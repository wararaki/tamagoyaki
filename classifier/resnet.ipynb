{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from cnn_finetune import make_model\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model('resnet18', num_classes=2, pretrained=True, input_size=(128, 128))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, csv_path, root_dir, transform=None):\n",
    "        self.data_df = pd.read_csv(csv_path)\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                mean=model.original_model_info.mean,\n",
    "                std=model.original_model_info.std),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = os.path.join(self.root_dir, self.data_df.iloc[idx, 0])\n",
    "        image = io.imread(image_name)\n",
    "        label = self.data_df.iloc[idx, 1]\n",
    "        return self.transform(image), int(label)\n",
    "    \n",
    "    def get_full_data(self):\n",
    "        image_names = [\n",
    "            os.path.join(self.root_dir, self.data_df.iloc[idx, 0]) for idx in range(self.data_df.shape[0])\n",
    "        ]\n",
    "        images = [io.imread(path) for path in image_names]\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataSet('../images/train_data.csv', '../images/resize/data/')\n",
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = MyDataSet('../images/test_data.csv', '../images/resize/data/')\n",
    "test_loader = torch.utils.data.DataLoader(test_set, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/creafz/pytorch-cnn-finetune/blob/master/examples/cifar10.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    total_loss = 0\n",
    "    total_size = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "        total_size += data.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 20 == 0 or (batch_idx+1)==len(train_loader):\n",
    "            progress = 100. * (batch_idx+1) / len(train_loader)\n",
    "            average_loss = total_loss / total_size\n",
    "            print(f'Train Epoch: {epoch} [{(batch_idx+1) * len(data)}/{len(train_loader.dataset)} ({progress:.0f}%)]\\tAverage loss: {average_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            targets.append(target)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
    "            outputs.append(output.cpu().detach().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n')\n",
    "    return outputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [20/173 (12%)]\tAverage loss: 5.133770\n",
      "Train Epoch: 1 [40/173 (23%)]\tAverage loss: 7.711930\n",
      "Train Epoch: 1 [60/173 (35%)]\tAverage loss: 6.867764\n",
      "Train Epoch: 1 [80/173 (46%)]\tAverage loss: 5.515359\n",
      "Train Epoch: 1 [100/173 (58%)]\tAverage loss: 5.176289\n",
      "Train Epoch: 1 [120/173 (69%)]\tAverage loss: 4.552049\n",
      "Train Epoch: 1 [140/173 (81%)]\tAverage loss: 4.091651\n",
      "Train Epoch: 1 [160/173 (92%)]\tAverage loss: 3.767900\n",
      "Train Epoch: 1 [173/173 (100%)]\tAverage loss: 3.560872\n",
      "\n",
      "Test set: Average loss: 0.6735, Accuracy: 45/76 (59%)\n",
      "\n",
      "Train Epoch: 2 [20/173 (12%)]\tAverage loss: 0.876862\n",
      "Train Epoch: 2 [40/173 (23%)]\tAverage loss: 4.307245\n",
      "Train Epoch: 2 [60/173 (35%)]\tAverage loss: 5.354260\n",
      "Train Epoch: 2 [80/173 (46%)]\tAverage loss: 4.344302\n",
      "Train Epoch: 2 [100/173 (58%)]\tAverage loss: 4.134697\n",
      "Train Epoch: 2 [120/173 (69%)]\tAverage loss: 3.908905\n",
      "Train Epoch: 2 [140/173 (81%)]\tAverage loss: 3.867714\n",
      "Train Epoch: 2 [160/173 (92%)]\tAverage loss: 3.490555\n",
      "Train Epoch: 2 [173/173 (100%)]\tAverage loss: 3.383153\n",
      "\n",
      "Test set: Average loss: 0.6623, Accuracy: 45/76 (59%)\n",
      "\n",
      "Train Epoch: 3 [20/173 (12%)]\tAverage loss: 1.126006\n",
      "Train Epoch: 3 [40/173 (23%)]\tAverage loss: 1.303645\n",
      "Train Epoch: 3 [60/173 (35%)]\tAverage loss: 1.127259\n",
      "Train Epoch: 3 [80/173 (46%)]\tAverage loss: 1.229728\n",
      "Train Epoch: 3 [100/173 (58%)]\tAverage loss: 1.254775\n",
      "Train Epoch: 3 [120/173 (69%)]\tAverage loss: 1.347135\n",
      "Train Epoch: 3 [140/173 (81%)]\tAverage loss: 1.660407\n",
      "Train Epoch: 3 [160/173 (92%)]\tAverage loss: 1.877347\n",
      "Train Epoch: 3 [173/173 (100%)]\tAverage loss: 2.074252\n",
      "\n",
      "Test set: Average loss: 0.8097, Accuracy: 45/76 (59%)\n",
      "\n",
      "Train Epoch: 4 [20/173 (12%)]\tAverage loss: 1.768247\n",
      "Train Epoch: 4 [40/173 (23%)]\tAverage loss: 1.615528\n",
      "Train Epoch: 4 [60/173 (35%)]\tAverage loss: 1.677868\n",
      "Train Epoch: 4 [80/173 (46%)]\tAverage loss: 1.553807\n",
      "Train Epoch: 4 [100/173 (58%)]\tAverage loss: 1.513558\n",
      "Train Epoch: 4 [120/173 (69%)]\tAverage loss: 1.412110\n",
      "Train Epoch: 4 [140/173 (81%)]\tAverage loss: 1.494724\n",
      "Train Epoch: 4 [160/173 (92%)]\tAverage loss: 1.469351\n",
      "Train Epoch: 4 [173/173 (100%)]\tAverage loss: 1.496736\n",
      "\n",
      "Test set: Average loss: 0.9041, Accuracy: 45/76 (59%)\n",
      "\n",
      "Train Epoch: 5 [20/173 (12%)]\tAverage loss: 1.330080\n",
      "Train Epoch: 5 [40/173 (23%)]\tAverage loss: 1.364792\n",
      "Train Epoch: 5 [60/173 (35%)]\tAverage loss: 1.339907\n",
      "Train Epoch: 5 [80/173 (46%)]\tAverage loss: 1.677391\n",
      "Train Epoch: 5 [100/173 (58%)]\tAverage loss: 1.874436\n",
      "Train Epoch: 5 [120/173 (69%)]\tAverage loss: 1.913754\n",
      "Train Epoch: 5 [140/173 (81%)]\tAverage loss: 1.812704\n",
      "Train Epoch: 5 [160/173 (92%)]\tAverage loss: 1.708799\n",
      "Train Epoch: 5 [173/173 (100%)]\tAverage loss: 1.723571\n",
      "\n",
      "Test set: Average loss: 0.7997, Accuracy: 31/76 (41%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
